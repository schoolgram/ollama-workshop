{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an LLM?\n",
    "### Definition: A large language model (LLM) is a deep learning model trained on a vast amount of textual data, designed to understand and generate human-like language.\n",
    "<img src=\"./What_is_llm.png\" width=\"700\" height=\"600\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "# Why Canâ€™t We Run LLMs Directly on Consumer Hardware?\n",
    "### Resource Requirements: The massive size (billions of parameters), memory constraints, and compute power needed for inference and training.\n",
    "### Performance Bottlenecks: GPU/CPU limitations, latency, and cost factors.\n",
    "### Example: Challenges of running models like GPT-3 on standard devices.\n",
    "<img src=\"./llm_hardware.webp\" width=\"700\" height=\"600\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "# What is Ollama?\n",
    "### Introduction to Ollama: A platform designed to efficiently run large language models on consumer-grade hardware.\n",
    "### Key Features: Support for quantization, optimized inference, and simplified deployment workflows.\n",
    "\n",
    "<img src=\"./what_is_ollama.webp\" width=\"700\" height=\"600\">\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "# How Does Ollama Make Running LLMs Easier? The concept of Quantization\n",
    "### Optimizations: Memory management, efficient parallelization, and reduced latency.\n",
    "<img src=\"./quantization.png\" width=\"700\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Ollama on macOS\n",
    "\n",
    "Welcome to the \"Getting Started with Ollama\" notebook! In this notebook, we'll cover the installation, setup, and basic usage of Ollama, a powerful tool for deploying large language models on macOS.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Prerequisites](#prerequisites)\n",
    "2. [Installation](#installation)\n",
    "3. [Basic Usage](#basic-usage)\n",
    "4. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following:\n",
    "\n",
    "- **Python 3.7 or higher** installed on your machine.\n",
    "- A working **terminal** application (you can use the built-in Terminal app).\n",
    "\n",
    "To check your Python version, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "python3 --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install Ollama on macOS, you can use Homebrew, a popular package manager.\n",
    "\n",
    "### Step 1: Install Homebrew (if not already installed)\n",
    "\n",
    "If you haven't installed Homebrew yet, run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Install Ollama\n",
    "Once Homebrew is installed, run the following command to install Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Verify the Installation\n",
    "To check if Ollama is installed correctly, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the Model\n",
    "After pulling the model, you can run it with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Now that you have Ollama installed, let's explore some basic commands.\n",
    "\n",
    "### Step 1: Pull a Model\n",
    "\n",
    "You can view a list of all pre-trained models available [here](https://ollama.com/library) but make sure to verify the RAM requirements.\n",
    "\n",
    "\n",
    "You can pull a pre-trained model Llama 3.2 (3B parameters and requires 2GB RAM) using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama run llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with the model through the terminal.\n",
    "\n",
    "\n",
    "Try asking it a question or giving it a prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1: Automating Jira Tickets Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Obtain your meeting transcription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcription.txt\", \"r\") as f:\n",
    "    transcription = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract tasks from Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT = \"\"\"Please analyze the following transcription of a conversation between different team members and extract actionable items or tasks. Return the extracted tasks in JSON format only, without any additional text or commentary. Each task should include a summary, a description, and the name of the assignee.\n",
    "\n",
    "Transcription: {transcription}\n",
    "\n",
    "Expected JSON Response:\n",
    "\n",
    "[\n",
    "    {{\n",
    "        \"summary\": \"Task summary here\",\n",
    "        \"description\": \"Detailed description of the task\",\n",
    "        \"assignee\": \"Name of the assignee\"\n",
    "    }},\n",
    "    {{\n",
    "        \"summary\": \"Another task summary here\",\n",
    "        \"description\": \"Detailed description of the second task\",\n",
    "        \"assignee\": \"Name of the second assignee\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "Make sure that the JSON response contains all relevant tasks discussed in the conversation and no other text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='llama3.2:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': PROMPT.format(transcription=transcription),\n",
    "  },],\n",
    "    stream=True)\n",
    "\n",
    "final_response = []\n",
    "for chunk in response:\n",
    "    print(chunk['message']['content'], end=\"\")\n",
    "    final_response.append(chunk['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = \"\".join(final_response)\n",
    "\n",
    "try:\n",
    "    tasks = json.loads(final_response)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Invalid JSON response\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get Jira Project Details\n",
    "[Jira REST API Documentation](https://developer.atlassian.com/cloud/jira/platform/rest/v3/intro/#version)\n",
    "\n",
    "\n",
    "[Get API Token](https://id.atlassian.com/manage-profile/security/api-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary variables\n",
    "JIRA_URL = \"\"\n",
    "\n",
    "EMAIL = \"\"\n",
    "API_TOKEN = \"\"\n",
    "\n",
    "# You need any one of the following\n",
    "PROJECT_KEY = \"SCRUM\"\n",
    "PROJECT_ID = \"10000\"\n",
    "\n",
    "# Account IDs of your team members (use this query: https://beinghasnain16.atlassian.net/rest/api/2/users/search?query)\n",
    "ASSIGNEES = {\"Hasnain\": \"\",\n",
    "             \"Zohaib\": \"\"}\n",
    "\n",
    "# Define the Issue type and Attributes\n",
    "ISSUE_DATA= { \"fields\": {\n",
    "        \"project\": {\n",
    "            \"id\": PROJECT_ID,\n",
    "        },\n",
    "        \"summary\": \"\",\n",
    "        \"description\": \"\",\n",
    "        \"issuetype\": {\n",
    "            \"name\": \"Task\"\n",
    "        },\n",
    "        \"assignee\": {\n",
    "            \"id\": \"\"\n",
    "        },\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request to create the issue\n",
    "for task in tasks:\n",
    "    issue_data = ISSUE_DATA.copy()\n",
    "    issue_data[\"fields\"]['summary'] = task[\"summary\"]\n",
    "    issue_data[\"fields\"]['description'] = task[\"description\"]\n",
    "    issue_data[\"fields\"]['assignee']['id'] = ASSIGNEES.get(task[\"assignee\"], \"\")\n",
    "\n",
    "    response = requests.post(\n",
    "        JIRA_URL,\n",
    "        data=json.dumps(issue_data),\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        auth=HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "    )\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 201:\n",
    "        print(\"Task created successfully:\", response.json())\n",
    "    else:\n",
    "        print(\"Failed to create task:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2: Imitating the Apple AI - Building a simple local chatgpt available on all applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama pyperclip pynput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Importing the required libraries\n",
    "### Description of installed libraries\n",
    "#### ollama: Ollama's Python sdk to deal with locally running ollama\n",
    "#### pyperclip: Deals with the laptop's clipboard. Copy to clipboard, Paste from clipboard etc\n",
    "#### pynput: Used to control the keyboard inputs and use copy pasting shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyperclip\n",
    "import ollama\n",
    "\n",
    "from string import Template\n",
    "from pynput import keyboard\n",
    "from pynput.keyboard import Key, Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initializing the Controller (to control keyboard input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = Controller()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Defining the templates that will be used later by the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = Template(\n",
    "    \"\"\"Fix all typos and casing and punctuation in this text, but preserve all new line characters:\n",
    "\n",
    "$text\n",
    "\n",
    "Return only the corrected text, don't include a preamble.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE_PERSONALIZED = Template(\n",
    "    \"\"\"$prompt:$text\"\"\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE_CODING_ASSISTANT = Template(\n",
    "    \"\"\"\n",
    "    You are a helpful coding assistant who can code in any language. Below is a code snippet or an instruction for a code. If there's any error in the code fix it. If its just a comment for a code\n",
    "    Complete the function. Dont add anything extra, just write the code as if you were in a code editor. Dont add anything other than the code or comments. For comments use appropriate syntax for the language. Dont add an inverted commas for anything\n",
    "    $text\n",
    "    Here is an additional instruction $prompt\n",
    "    . dont even add the (''') in the start and end. Just only give the code and NOTHING else\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Function to call Ollama endpoint and stream the response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2:1b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for chunk in stream:\n",
    "        pyperclip.copy(chunk['message']['content'])\n",
    "        time.sleep(0.1)\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        with controller.pressed(Key.cmd):\n",
    "            controller.tap(\"v\")\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Different functions to handle different kind of commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text, prompt=None, type='text'):\n",
    "    prompt = PROMPT_TEMPLATE_PERSONALIZED.substitute(text=text, prompt=prompt) if prompt and type!= \"Code\" else PROMPT_TEMPLATE_CODING_ASSISTANT.substitute(text=text, prompt=prompt) if type==\"Code\" else PROMPT_TEMPLATE.substitute(text=text)\n",
    "    try:\n",
    "        print(\"Prompt\", prompt)\n",
    "        get_response(prompt)\n",
    "    except Exception as e:\n",
    "        print(\"Error\", e)\n",
    "        return\n",
    "\n",
    "def fix_current_line():\n",
    "    # macOS short cut to select current line: Cmd+Shift+Left\n",
    "    controller.press(Key.cmd)\n",
    "    controller.press(Key.shift)\n",
    "    controller.press(Key.left)\n",
    "\n",
    "    controller.release(Key.cmd)\n",
    "    controller.release(Key.shift)\n",
    "    controller.release(Key.left)\n",
    "\n",
    "    fix_selection()\n",
    "\n",
    "def fix_selection():\n",
    "    # 1. Copy selection to clipboard\n",
    "    with controller.pressed(Key.cmd):\n",
    "        controller.tap(\"c\")\n",
    "\n",
    "    # 2. Get the clipboard string\n",
    "    time.sleep(0.1)\n",
    "    text = pyperclip.paste()\n",
    "    print(\"Copied text\", text)\n",
    "\n",
    "    # 3. Fix string\n",
    "    if not text:\n",
    "        return\n",
    "    fix_text(text)\n",
    "\n",
    "def fix_selection_with_prompt(type='text'):\n",
    "    # 1. Copy selection to clipboard\n",
    "    with controller.pressed(Key.cmd):\n",
    "        controller.tap(\"c\")\n",
    "\n",
    "    # 2. Get the clipboard string\n",
    "    time.sleep(0.1)\n",
    "    text = pyperclip.paste()\n",
    "    try:\n",
    "        prompt_with_selection = text.split(\"..\")\n",
    "        prompt = prompt_with_selection[1]\n",
    "        selected_text = prompt_with_selection[0]\n",
    "    except:\n",
    "        prompt = \"\"\n",
    "        selected_text = text\n",
    "    if not text:\n",
    "        return\n",
    "    print(\"Prompt\", prompt)\n",
    "    print(\"Selected text\", selected_text)\n",
    "    fix_text(selected_text, prompt, type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Key Binding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_f7():\n",
    "    fix_current_line()\n",
    "\n",
    "def on_f8():\n",
    "    fix_selection()\n",
    "\n",
    "def on_f9():\n",
    "    fix_selection_with_prompt()\n",
    "\n",
    "def on_f10():\n",
    "    fix_selection_with_prompt('Code')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step: Continously running the assistant and binding different keys to different commands\n",
    "### Before running the final cell, Make sure to allow some permissions for your code editor\n",
    "#### Go to Settings -> Privacy and Security -> Accessibility and turn it on for your Code Editor\n",
    "#### Then again Privacy and Security -> Input Monitoring and turn it on for your Code Editor\n",
    "### These are the available commands\n",
    "### F7 : Fix the current line. (Move the cursor to end of line)\n",
    "### F8 : Fix the selected paragraph\n",
    "### F9 : Give a prompt along with the highlighted text (Can be used to write anything)\n",
    "### F10 : Coding Assistant with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with keyboard.GlobalHotKeys({\"<100>\": on_f7, \"<98>\": on_f8, \"<101>\": on_f9, \"<109>\": on_f10}) as h:\n",
    "    h.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the installation and basic usage of Ollama on macOS. With the provided commands, you can easily set up and start using language models in your projects.\n",
    "\n",
    "We also discussed how we can automate the Jira tickets creation process with LLMs.\n",
    "\n",
    "For more information, visit the official [Ollama documentation](https://ollama.com/docs).\n",
    "\n",
    "Happy coding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
